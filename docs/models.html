<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Models - Nüshu Character Retrieval-Augmented LLM Agent</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <script src="scripts.js" defer></script>
    <style>
        .model-card {
            background-color: white;
            border-radius: 8px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
            padding: 1.5rem;
            margin-bottom: 2rem;
        }
        
        .model-header {
            display: flex;
            align-items: center;
            margin-bottom: 1rem;
        }
        
        .model-icon {
            font-size: 2rem;
            color: var(--secondary-color);
            margin-right: 1rem;
        }
        
        .model-name {
            font-size: 1.5rem;
            margin: 0;
        }
        
        .parameter-count {
            font-size: 1rem;
            color: #666;
            margin-top: 0.25rem;
        }
        
        .model-details {
            margin: 1.5rem 0;
        }
        
        .tech-specs {
            background-color: #f8f9fa;
            border-radius: 8px;
            padding: 1rem;
            margin-top: 1rem;
        }
        
        .tech-specs h4 {
            margin-top: 0;
            color: var(--primary-color);
        }
        
        .tech-specs ul {
            list-style-type: none;
            padding-left: 0;
        }
        
        .tech-specs li {
            margin-bottom: 0.5rem;
            display: flex;
            align-items: flex-start;
        }
        
        .tech-specs li i {
            color: var(--secondary-color);
            margin-right: 0.5rem;
            margin-top: 0.25rem;
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1>Models</h1>
            <p class="subtitle">Technical details about the language models used in the project</p>
            <nav>
                <a href="index.html">Home</a> |
                <a href="about.html">About</a> |
                <a href="models.html">Models</a>
            </nav>
        </div>
    </header>

    <section class="main-content">
        <div class="container">
            <h2>Base Model: DeepSeek-R1-Distill-Qwen-1.5B</h2>
            
            <div class="model-card">
                <div class="model-header">
                    <div class="model-icon"><i class="fas fa-brain"></i></div>
                    <div>
                        <h3 class="model-name">DeepSeek-R1-Distill-Qwen-1.5B</h3>
                        <p class="parameter-count">1.5 Billion Parameters</p>
                    </div>
                </div>
                
                <div class="model-details">
                    <p>The generative backbone of our system is the DeepSeek-R1-Distill-Qwen-1.5B model, an open-source large language model designed for robust reasoning and factual accuracy. DeepSeek-R1 1.5B is pre-trained on a diverse, multilingual corpus and further distilled for computational efficiency, making it highly suitable for integration with retrieval-augmented frameworks and downstream domain adaptation.</p>
                    
                    <p>A key factor in selecting this model is its optimal balance between performance and resource requirements: with only 1.5 billion parameters, DeepSeek-R1 1.5B can be efficiently fine-tuned on consumer hardware with limited GPU memory (e.g., 8GB VRAM), while still delivering strong results in reasoning and knowledge-intensive tasks. This makes it the most capable open-source model for our use case, given the hardware constraints of our experimental environment.</p>
                </div>
                
                <div class="tech-specs">
                    <h4>Technical Specifications</h4>
                    <ul>
                        <li><i class="fas fa-microchip"></i> <strong>Architecture:</strong> Transformer-based with optimizations for resource efficiency</li>
                        <li><i class="fas fa-server"></i> <strong>Deployment:</strong> Compatible with 8GB consumer GPUs</li>
                        <li><i class="fas fa-globe"></i> <strong>Multilingual Support:</strong> Includes Chinese language capabilities</li>
                        <li><i class="fas fa-book"></i> <strong>Context Length:</strong> 4K tokens</li>
                        <li><i class="fas fa-link"></i> <strong>Hugging Face:</strong> <a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B" target="_blank">deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B</a></li>
                    </ul>
                </div>
            </div>
            
            <h2>Nüshu-LoRA: Domain-Specialized Fine-tuning</h2>
            
            <div class="model-card">
                <div class="model-header">
                    <div class="model-icon"><i class="fas fa-cogs"></i></div>
                    <div>
                        <h3 class="model-name">Nvshu-LoRA</h3>
                        <p class="parameter-count">~2.5 Million Trainable Parameters (LoRA)</p>
                    </div>
                </div>
                
                <div class="model-details">
                    <p>To specialize the base DeepSeek model for Nüshu analysis, we implemented Low-Rank Adaptation (LoRA) fine-tuning. This technique allows for efficient domain adaptation while preserving the model's general capabilities. The approach involved training only a small set of additional parameters while keeping the base model frozen.</p>
                    
                    <div class="image-container">
                        <img src="https://yushiran.github.io/picx-images-hosting/DNLP/Lora.2rvd35n8j2.webp" alt="LoRA Mechanism" class="content-image">
                        <p class="caption">Low-Rank Adaptation (LoRA) mechanism</p>
                    </div>
                </div>
                
                <div class="tech-specs">
                    <h4>Training Details</h4>
                    <ul>
                        <li><i class="fas fa-database"></i> <strong>Dataset Size:</strong> ~2,000 training examples</li>
                        <li><i class="fas fa-layer-group"></i> <strong>LoRA Rank:</strong> 8</li>
                        <li><i class="fas fa-sort-numeric-up"></i> <strong>Epochs:</strong> 3</li>
                        <li><i class="fas fa-tachometer-alt"></i> <strong>Learning Rate:</strong> 5e-4 with linear decay</li>
                        <li><i class="fas fa-microchip"></i> <strong>Hardware:</strong> NVIDIA RTX 4060 Laptop GPU (8GB VRAM)</li>
                        <li><i class="fas fa-link"></i> <strong>Hugging Face:</strong> <a href="https://huggingface.co/ShiranYu/nvshu_lora" target="_blank">ShiranYu/nvshu_lora</a></li>
                    </ul>
                </div>
            </div>
            
            <h2>Training Process</h2>
            
            <h3>Dataset Preparation</h3>
            <p>To create a high-quality dataset for fine-tuning our model on the Nüshu domain, we developed a semi-automated data generation pipeline that leverages the knowledge graph and large language models. Our dataset creation process combines structured data from the Neo4j knowledge graph with generative capabilities of the OpenAI API to produce contextually rich training examples.</p>
            
            <p>The dataset consists of question-context-response triplets specifically designed for instruction fine-tuning, covering character identification, translation between Nüshu and Chinese characters, pronunciation guidance, and explanation of cultural significance.</p>
            
            <div class="image-container">
                <img src="https://yushiran.github.io/picx-images-hosting/DNLP/lora_dataset.5q7n6nvi0i.webp" alt="LoRA Dataset Examples" class="content-image">
                <p class="caption">Example instances from the LoRA fine-tuning dataset</p>
            </div>
            
            <h3>Training Method</h3>
            <p>With our specialized dataset prepared, we implemented the Low-Rank Adaptation (LoRA) technique to efficiently fine-tune the DeepSeek-R1-Distill-Qwen-1.5B model. The fine-tuning was performed on a single NVIDIA RTX 4060 Laptop GPU with 8GB VRAM, which would not have been possible with full fine-tuning of even this relatively small 1.5B parameter model.</p>
            
            <p>The LoRA approach reduced the number of trainable parameters from 1.5 billion to approximately 2.5 million, a reduction of over 99%, enabling efficient training on consumer-grade hardware. We trained the model for 3 epochs on our dataset, with a 90-10 train-validation split.</p>
            
            <div class="image-container">
                <img src="https://yushiran.github.io/picx-images-hosting/DNLP/loss_curve.8z6r3bizn9.webp" alt="Validation Loss Curve" class="content-image">
                <p class="caption">Validation Loss Curve for Nvshu Fine-tuned Model</p>
            </div>
        </div>
    </section>

    <footer>
        <div class="container">
            <p>© 2025 Nüshu Character Retrieval-Augmented LLM Agent. Project developed as part of DLNLP Assignment.</p>
            <div class="footer-links">
                <a href="https://github.com/your-username/nushu-agent" target="_blank"><i class="fab fa-github"></i> GitHub</a>
                <a href="https://huggingface.co/ShiranYu/nvshu_lora" target="_blank"><i class="fas fa-brain"></i> Hugging Face</a>
            </div>
        </div>
    </footer>
</body>
</html>
